心得：
1.增加隱藏層(數量一致)，原來學1000次的效果約變1/4，但權重值(width)也會跟著變大

2.周期性更新資料，也能加快學習
	例.
		每次一個單字背十次
		每次十個單字背一次(勝)
	2.1 每次學習新資料時，loss會提高，但隨著訓練次數提高，loss提高的狀況越來越低(學習效果變好)
	2.2 訓練的準確率(acc)100%不代表真實準確率100％，還是要看驗証的準確率。而驗証準確率100%也只是樣本的準確率。
		10000樣本測出的99.999%準確率，還是會有1/10000的錯誤率，萬中選一的武林奇材
		
3.權重值記錄起來，之後也可以持續訓練與直接預測
	3.1 可調整(output/input)的長度
		大→小：準確率提升
		小→大：準確率下降
	3.2 調整隱藏層則會拋exception(記錄的權重值應該有記錄隱藏層的陣列長度)
		
4.用 one-hot encodes 做資料標準化
	可先把所有字元的狀況先做初始化(a-z, 0-9,其他符號字元)，之後的學習預測再encode與decode。
	好處是不用把資料標準化的內容記錄起來，
	壞處是學習中文相關的東西就囧了，中文字沒辨法做初始化(轉成acsii code或其他編碼再學習非常的麻煩)
